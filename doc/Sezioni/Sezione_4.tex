\newpage
\section{Discussion}\label{sec:target_of_the_project} 
In this section we'll discuss the result obtained at the end of the pipeline, splitting the analysis in three steps, in the first step we'll discuss about results on feature selection task, on the second step the discussion will focus on classification task, and the conclusion step will discuss about the evolution of the results and performance over time.
\subsection{Feature selection}\label{subsec:status}
Feature selection task over time highlight an alternations between dominant discriminant. The main dominant channels is FC2 and C4, but not always is the best choice for classifier. As example in session 20190502, the first run highlight FC2 and C4 on 22Hz, but at third run the discriminant on C4 is weak, and during the second run we can observe some movements in 20Hz band. This situation is also observable on 20190506 runs, where the beahaveior is similar at the second run of previous session.
This uncertainty continues to recur in subsequent sessions, making it difficult to identify a unique discriminant valid for all sessions. Also in some sessions, such as the second run of the 20190709 session, where the fischer score highlights the presence of discriminants around the FCz channel on lower frequency bands between 12Hz and 18Hz. Certainly the univocal choice of channels and a common frequency band simplifies the operations but strongly penalizes some sessions, in which there is no strong discrimination between the selected channels and frequencies.
\subsection{Classification}\label{subsec:status}
As explained in the previous step, the choice of a discriminant common to all the sessions penalized all those that after each run presented discriminants distant from the common ones. Or all the sessions in which between the various runs presented a large variance in the fisher score.
All the graphs representing the classifiers are able to give a minimum separation. But unfortunately without guaranteeing the efficient separation that we would have obtained using an ad hoc feature selection tailored to each single session. Absolutely useless to try with a single classifier, it would certainly enrich the large dataset, but without taking into account the evolution over time. 
As a consequence, the results obtained in the testing phase fail to give strong evidence, despite the fact that the threshold set at 0.7 is not stringent as a parameter.
\subsection{Evolution over time}\label{subsec:status}
The strength of a static learning system is the invariance of the subject over time. In this case the learning adapts to the subject, and at the same time during the online sessions it is the same subject to adapt to the learning model.
This makes the learning process more complex across sessions, the data used for the previous models become of little use to the development of the current one. The evolution and the great variance between the various offline sessions after the sessions with feedback, demonstrates how the pilot, helped from feedbacks, adapts to the model. The real difficulty and the big difference with reinforcement learning is that with each change of the pilot, the previously acquired data cannot always be used for the train of the new model. Especially during the month of June where there were few offline sessions, we were able to observe the strong changes during the feature selection, that required a recalibration. And above all highlighted the need to carry out both types of sessions continuously, in order to prevent similar gap between one session and another. Avoiding that the patient with online-only sessions, adapting to the model introduces new features that cause loss of precision in the current model.



